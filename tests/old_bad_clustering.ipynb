{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Add the root project directory to the Python path\n",
    "project_root = Path.cwd().parent  # This will get the project root since the notebook is in 'notebooks/'\n",
    "sys.path.append(str(project_root))\n",
    "from configs.path_config import EXTRACTED_DATA_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Description: Load data from a csv file containing strain distributions.\n",
    "\n",
    "    Args:\n",
    "        path (path object): The path to the csv file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.isna().sum().sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "path = EXTRACTED_DATA_DIR / 'strain_distributions' / 'alvbrodel_04' / 'S-B_Close_Comp_20091129120000_20210611160000_strain_distribution_04 1.csv'\n",
    "# path = OUTPUT_DIR / 'strain_distributions' / 'N-F_Mid_Comp_20091129120000_20210611160000_strain_distribution.csv'\n",
    "df = load_data(path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_by_header_rules(df, threshold):\n",
    "    first_col = df.columns[0]\n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in df.columns[1:]:  # Skip first column\n",
    "        # Check if the column name contains more than one dot\n",
    "        if str(col).count('.') > 1:\n",
    "            cols_to_drop.append(col)\n",
    "            continue\n",
    "\n",
    "        # Try to convert to float and check if above threshold\n",
    "        try:\n",
    "            if float(col) < threshold:\n",
    "                cols_to_drop.append(col)\n",
    "        except ValueError:\n",
    "            continue  # Skip if not convertible\n",
    "\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Example usage:\n",
    "df = drop_columns_by_header_rules(df, threshold=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, threshold, individual_threshold):\n",
    "    \"\"\"\n",
    "    Description: Remove outliers from the data in three ways:\n",
    "        1. Remove rows where the mean exceeds the threshold deviation from the overall mean based on absolute values.\n",
    "        2. Remove rows where any value in the row exceeds the threshold based on its own mean (using absolute values).\n",
    "        3. Remove individual values in rows that are too large in relation to the rest of the values along the same row.\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "        threshold (float): The threshold for determining outliers based on the mean. Default is 1.\n",
    "        individual_threshold (float): The threshold for individual values compared to the row's mean. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "        df (pd DataFrame): The data cleaned of outliers, with the timestamp column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows with any NaNs\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df_strain = df.drop(columns='Timestamp')\n",
    "    abs_df = df_strain.abs()  # Take the absolute values of the dataframe\n",
    "\n",
    "    # 1. Remove rows where the mean is above the threshold deviation from the overall mean (based on absolute values)\n",
    "    row_means = abs_df.mean(axis=1)\n",
    "    Q1 = row_means.quantile(0.25)\n",
    "    Q3 = row_means.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    mean_outliers = row_means[(row_means < lower_bound) | (row_means > upper_bound)]\n",
    "\n",
    "    print(\"Mean-based outliers using IQR:\")\n",
    "    print(mean_outliers)\n",
    "\n",
    "    # means = abs_df.mean(axis=1)  # Calculate mean of absolute values for each row\n",
    "    # mean_val = means.mean()  # Mean of all row means\n",
    "    # std_val = means.std()  # Standard deviation of the row means\n",
    "\n",
    "    # # Define a threshold for the mean-based outliers (absolute values)\n",
    "    # mean_threshold = threshold\n",
    "\n",
    "    # # Find outliers based on absolute mean deviation\n",
    "    # mean_outliers = means[np.abs(means - mean_val) > mean_threshold * std_val]\n",
    "    # print(\"Mean-based outliers (absolute values):\")\n",
    "    # print(mean_outliers)\n",
    "\n",
    "    # 2. Remove rows where any value exceeds the threshold based on the row's mean (using absolute values)\n",
    "    row_outliers = []\n",
    "    for idx, row in abs_df.iterrows():\n",
    "        row_mean = row.mean()  # Mean of the absolute values in the row\n",
    "        row_std = row.std()  # Standard deviation of the absolute values in the row\n",
    "        threshold_value = row_mean + threshold * row_std  # Define threshold for each row based on absolute values\n",
    "        \n",
    "        # Check if any value in the row exceeds the calculated threshold (absolute value)\n",
    "        if (row > threshold_value).any():\n",
    "            row_outliers.append(idx)  # Keep track of the outlier row indices\n",
    "    \n",
    "    print(\"Outliers based on row threshold (absolute values):\")\n",
    "    print(df_strain.loc[row_outliers])\n",
    "\n",
    "    # 3. Remove individual values that are too large in relation to the row mean\n",
    "    large_value_outliers = []\n",
    "    for idx, row in abs_df.iterrows():\n",
    "        row_mean = row.mean()  # Mean of the absolute values in the row\n",
    "        row_max = row.max()  # Max value in the row\n",
    "        \n",
    "        # Compare the maximum value to the row mean; if it exceeds the threshold, flag it\n",
    "        if row_max > individual_threshold * row_mean:\n",
    "            large_value_outliers.append(idx)\n",
    "    \n",
    "    print(\"Outliers based on individual large values in relation to row mean:\")\n",
    "    print(df_strain.loc[large_value_outliers])\n",
    "\n",
    "    # Combine all outliers (mean-based, row-based, and individual large value-based) and drop them\n",
    "    all_outliers = mean_outliers.index.union(row_outliers).union(large_value_outliers)\n",
    "    \n",
    "    # Remove the rows from the data\n",
    "    df_strain = df_strain.drop(all_outliers)  # Removed outliers without timestamp\n",
    "    df = df.drop(all_outliers)  # Removed outliers with timestamp\n",
    "    \n",
    "    print(f\"Total number of outliers removed: {len(all_outliers)}\")\n",
    "\n",
    "    return df_strain, df\n",
    "\n",
    "# Usage\n",
    "df_strain, df = remove_outliers(df, threshold=7, individual_threshold=7)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_outliers(df):\n",
    "#     \"\"\"\n",
    "#     Description: Remove outliers from the data by calculating the mean of each row and removing rows where the mean deviates\n",
    "#     from the overall mean.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd DataFrame): The data loaded from the csv file.\n",
    "\n",
    "#     Returns:\n",
    "#         df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "#         df (pd DataFrame): The data cleaned of outliers, with the timestamp column.\n",
    "#     \"\"\"\n",
    "#     df_strain = df.drop(columns = 'Timestamp')\n",
    "#     means = df_strain.mean(axis=1)  # Calculate mean for each row\n",
    "\n",
    "#     # Calculate the mean and standard deviation of the row means\n",
    "#     mean_val = means.mean()\n",
    "#     std_val = means.std()\n",
    "\n",
    "#     # Define a threshold for outliers (3 standard deviations from the mean)\n",
    "#     threshold = 1\n",
    "\n",
    "#     # Find outliers: rows where the absolute deviation from the mean is greater than the threshold\n",
    "#     outliers = means[np.abs(means - mean_val) > threshold * std_val]\n",
    "#     print(\"Outliers:\")\n",
    "#     print(outliers)\n",
    "\n",
    "#     df_strain = df_strain.drop(outliers.index) #Removed outliers without timestamp\n",
    "#     df = df.drop(outliers.index) #Removed outliers with timestamp\n",
    "#     print(\"Number of outliers removed: \", len(outliers))\n",
    "\n",
    "#     return df_strain, df\n",
    "\n",
    "# df_strain, df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform PCA on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_data(df_strain):\n",
    "#     \"\"\"\n",
    "#     Description: Scale the data using MinMaxScaler.\n",
    "\n",
    "#     Args:\n",
    "#         df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "\n",
    "#     Returns:\n",
    "#         df_strain (pd DataFrame): The scaled data.\n",
    "#     \"\"\"\n",
    "#     # Exclude timestamp in column 0\n",
    "#     df_strain = df_strain.iloc[:, 1:]\n",
    "\n",
    "#     # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaler = StandardScaler()\n",
    "#     df_strain = pd.DataFrame(scaler.fit_transform(df_strain.T).T, columns=df_strain.columns)\n",
    "\n",
    "#     return df_strain\n",
    "\n",
    "# df_strain = scale_data(df_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_variance(df_strain) -> None:  \n",
    "    \"\"\"\n",
    "    Descripition: Plots the explained variance by number of components for PCA.\n",
    "\n",
    "    Args:\n",
    "        df_strain (pd DataFrame): The data.\n",
    "\n",
    "    Returns:\n",
    "        None    \n",
    "    \"\"\"\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    # Set the number of components directly (e.g., 5 components)\n",
    "    pca = PCA(n_components=10)\n",
    "    pca.fit(df_strain)\n",
    "    # \n",
    "    # Get the explained variance ratio\n",
    "    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "    # Plot the cumulative explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(per_var) + 1), per_var.cumsum(), marker=\"o\", linestyle=\"--\")\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Percentage Cumulative of Explained Variance\")\n",
    "    plt.xlabel(\"Number of Principal Components\")\n",
    "    plt.xticks(range(1, len(per_var) + 1, 1))\n",
    "    plt.title(\"Explained Variance by Number of Components\")\n",
    "    plt.show()\n",
    "\n",
    "explain_variance(df_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, df_strain):\n",
    "    \"\"\"\n",
    "    Description: Perform PCA on the data.\n",
    "\n",
    "    Args:\n",
    "        n_components (int): The number of principal components to keep.\n",
    "        df_strain (pd DataFrame): The data.\n",
    "\n",
    "    Returns:\n",
    "        df_pca (pd DataFrame): The principal components for each timestamp.\n",
    "        normalized_pca_components (np array) : The normalized principal components.\n",
    "    \"\"\"\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    pca.fit(df_strain)\n",
    "\n",
    "    # Apply PCA to the entire strain data (matrix-wise)\n",
    "    pca_results = pca.transform(df_strain)\n",
    "\n",
    "    # Normalize the results\n",
    "    normalized_pca_components = StandardScaler().fit_transform(pca_results)\n",
    "    # normalized_pca_components = MinMaxScaler().fit_transform(pca_results)\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df_pca = pd.DataFrame(normalized_pca_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Add timestamps back\n",
    "    df_pca.insert(0, 'Timestamp', df['Timestamp'].values)       #2009-11-30 040000\n",
    "    # df_pca.insert(0, '2009-11-30 040000', df['2009-11-30 040000'].values)\n",
    "\n",
    "    return normalized_pca_components, df_pca\n",
    "\n",
    "n_components = 10\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kmeans_clustering(normalized_pca_components, df, n_clusters):\n",
    "#     \"\"\"\n",
    "#     Description: Perform K-Means clustering on the normalized PCA components and visualize the clusters.\n",
    "#     Args:\n",
    "#         normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "#         df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "#         n_clusters (int): The number of clusters to create.\n",
    "\n",
    "#     Returns:\n",
    "#         data_with_KMeans (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "#     \"\"\"\n",
    "   \n",
    "#     kmeans = KMeans(n_clusters, random_state=42)\n",
    "#     clusters = kmeans.fit_predict(normalized_pca_components)\n",
    "\n",
    "#     # Add cluster labels to your original data (without overwriting)\n",
    "#     data_with_KMeans = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "#     # Insert the clusters as the second column (at index 1)\n",
    "#     data_with_KMeans.insert(1, 'Cluster', clusters)\n",
    "\n",
    "#     # Count the number of data points assigned to each cluster\n",
    "#     cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "#     # Plot the clusters\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     scatter = sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "#     # Create custom labels for the legend with the cluster counts\n",
    "#     legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "#     handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "#     # Set the custom labels in the legend\n",
    "#     plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "#     # Label the axes\n",
    "#     plt.xlabel(\"Principal Component 1\")\n",
    "#     plt.ylabel(\"Principal Component 2\")\n",
    "#     plt.title(\"PCA + KMeans Clustering\")\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "#     # Show the updated DataFrame with the Cluster column as the second column\n",
    "#     return data_with_KMeans\n",
    "\n",
    "# n_components = 10\n",
    "# normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "# n_clusters = 8\n",
    "# data_with_KMeans = kmeans_clustering(normalized_pca_components, df, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the cluster assignment over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_over_time(data_with_clusters, method) -> None:\n",
    "    \"\"\"\n",
    "    Plot the assignment to clusters over time, only displaying active clusters.\n",
    "    \n",
    "    Args:\n",
    "        data_with_clusters (pd.DataFrame): DataFrame with 'Timestamp' and 'Cluster' columns.\n",
    "        method (str): Name of the clustering method for the plot title.\n",
    "    \"\"\"\n",
    "    # Convert timestamps\n",
    "    data_with_clusters['Timestamp'] = pd.to_datetime(data_with_clusters['Timestamp'])\n",
    "\n",
    "    # Identify active clusters (those actually used)\n",
    "    active_clusters = sorted(data_with_clusters['Cluster'].dropna().unique())\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_with_clusters['Timestamp'], \n",
    "        y=data_with_clusters['Cluster'], \n",
    "        mode='markers+lines',\n",
    "        marker=dict(\n",
    "            size=6, \n",
    "            color=data_with_clusters['Cluster'], \n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title='Cluster')\n",
    "        ),\n",
    "        line=dict(width=0.5, color='gray')\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Assignment to Clusters Over Time with {method} Clustering',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Cluster',\n",
    "        xaxis_tickangle=-45,\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=active_clusters,\n",
    "            ticktext=[str(c) for c in active_clusters]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "# plot_clusters_over_time(data_with_KMeans, 'KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the mean distribution for each cluster and the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_mean_and_std(data_with_clusters, clusters_to_keep, method) -> None:\n",
    "    \"\"\"\n",
    "    Plot the mean strain values for each cluster with uncertainty (standard deviation).\n",
    "\n",
    "    Args:\n",
    "        data_with_clusters (pd.DataFrame): DataFrame with 'Timestamp' and 'Cluster' columns,\n",
    "                                           other columns should be numeric sensor data (e.g., distances).\n",
    "        clusters_to_keep (list): List of cluster labels to keep (e.g., [0, 1, 2]) or ['all'] to keep all.\n",
    "        method (str): Name of clustering method for plot title.\n",
    "    \"\"\"\n",
    "    # Get unique clusters and normalize types\n",
    "    cluster_col = data_with_clusters['Cluster'].dropna()\n",
    "    unique_clusters = cluster_col.astype(str).unique()\n",
    "\n",
    "    # Normalize user input\n",
    "    if clusters_to_keep == ['all']:\n",
    "        clusters_to_keep = unique_clusters\n",
    "    else:\n",
    "        clusters_to_keep = [str(c) for c in clusters_to_keep]\n",
    "\n",
    "    # Drop timestamp for clustering-related stats\n",
    "    df_mean = data_with_clusters.drop(columns='Timestamp').groupby('Cluster').mean()\n",
    "    df_std = data_with_clusters.drop(columns='Timestamp').groupby('Cluster').std()\n",
    "\n",
    "    # Filter to desired clusters\n",
    "    df_mean = df_mean.loc[df_mean.index.astype(str).isin(clusters_to_keep)]\n",
    "    df_std = df_std.loc[df_std.index.astype(str).isin(clusters_to_keep)]\n",
    "\n",
    "    x_values = pd.to_numeric(df_mean.columns, errors='coerce')\n",
    "    average_std = df_std.mean(axis=1)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    for i, cluster in enumerate(df_mean.index):\n",
    "        plt.plot(x_values, df_mean.loc[cluster], label=f'Cluster {cluster} - Mean, Avg. std: {average_std[cluster]:.2f}', linewidth=2)\n",
    "        plt.fill_between(x_values,\n",
    "                         df_mean.loc[cluster] - df_std.loc[cluster],\n",
    "                         df_mean.loc[cluster] + df_std.loc[cluster],\n",
    "                         alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Distance [m]')\n",
    "    plt.ylabel('Strain (Mean Value)')\n",
    "    plt.title(f'{method} Clustering Centroids with Uncertainty (Standard Deviation)')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# clusters_to_keep = ['all'] # 'all' or a list of cluster indices\n",
    "# plot_cluster_mean_and_std(data_with_KMeans, clusters_to_keep, 'KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gmm clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(normalized_pca_components, df, n_clusters):\n",
    "    \"\"\"\n",
    "    Description: Perform GMM clustering on the normalized PCA components and visualize the clusters.\n",
    "    Args:\n",
    "        normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "        df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "        n_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        data_with_gmm (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "    \"\"\"\n",
    "   \n",
    "    gmm = GaussianMixture(n_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(normalized_pca_components)\n",
    "\n",
    "    # Extract cluster probabilities\n",
    "    probabilities = gmm.predict_proba(normalized_pca_components)\n",
    "\n",
    "    # Get the probability of the assigned cluster\n",
    "    assigned_prob = probabilities[np.arange(len(clusters)), clusters]\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_gmm = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_gmm.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    data_with_gmm.insert(2, 'Assigned_Cluster_Prob', assigned_prob)  # Insert probability for the assigned cluster\n",
    "\n",
    "\n",
    "    # Count the number of data points assigned to each cluster\n",
    "    cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Create custom labels for the legend with the cluster counts\n",
    "    legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Set the custom labels in the legend\n",
    "    plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + GMM Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_gmm\n",
    "\n",
    "# n_components = 10\n",
    "# normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "n_clusters = 10\n",
    "data_with_gmm = gmm_clustering(normalized_pca_components, df, n_clusters)\n",
    "data_with_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_over_time(data_with_gmm, 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_to_keep = [8, 9] # 'all' or a list of cluster indices\n",
    "plot_cluster_mean_and_std(data_with_gmm, clusters_to_keep, 'GMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLIDING WINDOW DPGMM clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "# === Symmetric KL divergence (Jeffrey's divergence) ===\n",
    "def kl_divergence(mu0, cov0, mu1, cov1):\n",
    "    d = mu0.shape[0]\n",
    "    cov1_inv = np.linalg.inv(cov1)\n",
    "    trace_term = np.trace(cov1_inv @ cov0)\n",
    "    diff = mu1 - mu0\n",
    "    quad_term = diff.T @ cov1_inv @ diff\n",
    "    log_det_term = np.log(np.linalg.det(cov1) / np.linalg.det(cov0))\n",
    "    return 0.5 * (trace_term + quad_term - d + log_det_term)\n",
    "\n",
    "def jeffreys_divergence(mu0, cov0, mu1, cov1):\n",
    "    return 0.5 * (kl_divergence(mu0, cov0, mu1, cov1) + kl_divergence(mu1, cov1, mu0, cov0))\n",
    "\n",
    "def merge_clusters_by_divergence(dpgmm, labels, threshold):\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) <= 1:\n",
    "        return labels\n",
    "\n",
    "    means = dpgmm.means_\n",
    "    covariances = dpgmm.covariances_\n",
    "    label_indices = [label for label in unique_labels if np.sum(labels == label) > 0]\n",
    "\n",
    "    distance_matrix = np.zeros((len(label_indices), len(label_indices)))\n",
    "    for i, idx_i in enumerate(label_indices):\n",
    "        for j, idx_j in enumerate(label_indices):\n",
    "            if i < j:\n",
    "                dist = jeffreys_divergence(means[idx_i], covariances[idx_i], means[idx_j], covariances[idx_j])\n",
    "                distance_matrix[i, j] = distance_matrix[j, i] = dist\n",
    "\n",
    "    # Compute condensed distance matrix\n",
    "    condensed_distance = squareform(distance_matrix, checks=False)\n",
    "    Z = linkage(condensed_distance, method='average')\n",
    "    new_cluster_ids = fcluster(Z, t=threshold, criterion='distance')\n",
    "    label_map = {old: new for old, new in zip(label_indices, new_cluster_ids)}\n",
    "    merged_labels = np.array([label_map.get(label, -1) for label in labels])\n",
    "\n",
    "    return merged_labels\n",
    "\n",
    "\n",
    "def streaming_dpgmm_clustering(normalized_pca_components, df, n_points, window_size, step_size, max_components, merge_threshold, merge_within_window):\n",
    "    prior = 1\n",
    "    all_labels = np.full(len(df), -1)\n",
    "    all_probs = np.zeros(len(df))\n",
    "    all_results = []\n",
    "\n",
    "    # === Initial fit ===\n",
    "    initial_data = normalized_pca_components[:n_points]\n",
    "    dpgmm_init = BayesianGaussianMixture(\n",
    "        n_components=max_components,\n",
    "        covariance_type='full',\n",
    "        weight_concentration_prior_type='dirichlet_process',\n",
    "        weight_concentration_prior=prior,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        init_params='kmeans',\n",
    "        random_state=42\n",
    "    )\n",
    "    dpgmm_init.fit(initial_data)\n",
    "    initial_labels = dpgmm_init.predict(initial_data)\n",
    "    if merge_within_window:\n",
    "        initial_labels = merge_clusters_by_divergence(dpgmm_init, initial_labels, merge_threshold)\n",
    "    initial_probs = dpgmm_init.predict_proba(initial_data)\n",
    "    initial_max_probs = initial_probs[np.arange(len(initial_labels)), initial_labels]\n",
    "    all_labels[:n_points] = initial_labels\n",
    "    all_probs[:n_points] = initial_max_probs\n",
    "\n",
    "    print(f\"Initial fit => Clusters used: {np.sum(dpgmm_init.weights_ > 0.01)}\")\n",
    "\n",
    "    # === Streaming windows ===\n",
    "    for start in range(n_points, len(df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window_data = normalized_pca_components[start:end]\n",
    "        memory_data = normalized_pca_components[:end]\n",
    "\n",
    "        dpgmm = BayesianGaussianMixture(\n",
    "            n_components=max_components,\n",
    "            covariance_type='full',\n",
    "            weight_concentration_prior_type='dirichlet_process',\n",
    "            weight_concentration_prior=prior,\n",
    "            max_iter=1500,\n",
    "            tol=1e-3,\n",
    "            init_params='kmeans',\n",
    "            random_state=42\n",
    "        )\n",
    "        dpgmm.fit(memory_data)\n",
    "\n",
    "        labels = dpgmm.predict(window_data)\n",
    "        if merge_within_window:\n",
    "            labels = merge_clusters_by_divergence(dpgmm, labels, merge_threshold)\n",
    "\n",
    "        probs = dpgmm.predict_proba(window_data)\n",
    "        max_probs = probs[np.arange(len(labels)), labels]\n",
    "\n",
    "        all_labels[start:end] = labels\n",
    "        all_probs[start:end] = max_probs\n",
    "\n",
    "        active_clusters = np.sum(dpgmm.weights_ > 0.01)\n",
    "        print(f\"Window {start}-{end} => Active clusters: {active_clusters}, Top 5 weights: {np.round(dpgmm.weights_[:5], 3)}\")\n",
    "\n",
    "        all_results.append({\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'labels': labels,\n",
    "            'probs': max_probs\n",
    "        })\n",
    "\n",
    "    # === Final window ===\n",
    "    final_start = start + step_size\n",
    "    if final_start < len(df):\n",
    "        final_data = normalized_pca_components[final_start:]\n",
    "        memory_data = normalized_pca_components[:]\n",
    "\n",
    "        dpgmm_final = BayesianGaussianMixture(\n",
    "            n_components=max_components,\n",
    "            covariance_type='full',\n",
    "            weight_concentration_prior_type='dirichlet_process',\n",
    "            weight_concentration_prior=prior,\n",
    "            max_iter=1000,\n",
    "            tol=1e-3,\n",
    "            init_params='kmeans',\n",
    "            random_state=42\n",
    "        )\n",
    "        dpgmm_final.fit(memory_data)\n",
    "\n",
    "        final_labels = dpgmm_final.predict(final_data)\n",
    "        if merge_within_window:\n",
    "            final_labels = merge_clusters_by_divergence(dpgmm_final, final_labels, merge_threshold)\n",
    "\n",
    "        final_probs = dpgmm_final.predict_proba(final_data)\n",
    "        final_max_probs = final_probs[np.arange(len(final_labels)), final_labels]\n",
    "\n",
    "        all_labels[final_start:] = final_labels\n",
    "        all_probs[final_start:] = final_max_probs\n",
    "\n",
    "        print(f\"Final window {final_start}-{len(df)} => Active clusters: {np.sum(dpgmm_final.weights_ > 0.01)}\")\n",
    "\n",
    "        all_results.append({\n",
    "            'start': final_start,\n",
    "            'end': len(df),\n",
    "            'labels': final_labels,\n",
    "            'probs': final_max_probs\n",
    "        })\n",
    "    else:\n",
    "        dpgmm_final = dpgmm  # fallback\n",
    "\n",
    "    # === Merge clusters outside the window loop === (if enabled)\n",
    "    if not merge_within_window:\n",
    "            # === Merge Clusters using Jeffrey's Divergence ===\n",
    "        unique_labels = np.unique(all_labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels = unique_labels[unique_labels != -1]\n",
    "\n",
    "        if len(unique_labels) > 1:\n",
    "            means = dpgmm_final.means_\n",
    "            covariances = dpgmm_final.covariances_\n",
    "            label_indices = [label for label in unique_labels if np.sum(all_labels == label) > 0]\n",
    "\n",
    "            distance_matrix = np.zeros((len(label_indices), len(label_indices)))\n",
    "            for i, idx_i in enumerate(label_indices):\n",
    "                for j, idx_j in enumerate(label_indices):\n",
    "                    if i < j:\n",
    "                        dist = jeffreys_divergence(means[idx_i], covariances[idx_i], means[idx_j], covariances[idx_j])\n",
    "                        distance_matrix[i, j] = distance_matrix[j, i] = dist\n",
    "\n",
    "        sns.heatmap(distance_matrix, cmap='viridis')\n",
    "        plt.title(\"Jeffrey's Divergence Between Cluster Gaussians\")\n",
    "        plt.show()\n",
    "\n",
    "        Z = linkage(distance_matrix, method='average')\n",
    "        new_cluster_ids = fcluster(Z, t=merge_threshold, criterion='distance')\n",
    "        label_map = {old: new for old, new in zip(label_indices, new_cluster_ids)}\n",
    "        all_labels = np.array([label_map.get(label, -1) for label in all_labels])\n",
    "\n",
    "\n",
    "    # === Return DataFrame with results ===\n",
    "    df_result = df.copy()\n",
    "    df_result.insert(1, 'Cluster', all_labels)\n",
    "    df_result.insert(2, 'Assigned_Cluster_Prob', all_probs)\n",
    "\n",
    "    used_clusters = np.unique(all_labels)\n",
    "    used_clusters = used_clusters[used_clusters != -1]\n",
    "    palette = sns.color_palette('viridis', len(used_clusters))\n",
    "    cluster_color_map = {label: palette[i] for i, label in enumerate(used_clusters)}\n",
    "    cluster_color_map[-1] = (0.6, 0.6, 0.6)\n",
    "\n",
    "    # === Plot clusters with counts in legend ===\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.scatterplot(\n",
    "        x=normalized_pca_components[:, 0],\n",
    "        y=normalized_pca_components[:, 1],\n",
    "        hue=all_labels,\n",
    "        palette=cluster_color_map,\n",
    "        s=60,\n",
    "        alpha=0.7,\n",
    "        legend='full'\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"Streaming PCA + DPGMM Clustering\")\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    label_counts = pd.Series(all_labels).value_counts().sort_index()\n",
    "\n",
    "    new_labels = []\n",
    "    for lbl in labels:\n",
    "        try:\n",
    "            cluster_id = int(lbl)\n",
    "            count = label_counts.get(cluster_id, 0)\n",
    "            new_labels.append(f\"Cluster {cluster_id} ({count} samples)\")\n",
    "        except ValueError:\n",
    "            new_labels.append(lbl)\n",
    "\n",
    "    ax.legend(handles=handles, labels=new_labels, title='Cluster', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    return df_result, all_results\n",
    "\n",
    "\n",
    "data_with_dpgmm, _ = streaming_dpgmm_clustering(\n",
    "    normalized_pca_components=normalized_pca_components,\n",
    "    df=df,\n",
    "    n_points=1000,\n",
    "    window_size=400,\n",
    "    step_size=100,\n",
    "    max_components=100,\n",
    "    merge_threshold=60,\n",
    "    merge_within_window=True  # Toggle ON/OFF\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_over_time(data_with_dpgmm, 'DPGMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters_to_keep = [1, 2] # 'all' or a list of cluster indices\n",
    "plot_cluster_mean_and_std(data_with_dpgmm, clusters_to_keep, method='DPGMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "def evaluate_clustering_quality(features, labels):\n",
    "    # Filter out unassigned data points (label -1)\n",
    "    mask = labels != -1\n",
    "    filtered_features = features[mask]\n",
    "    filtered_labels = labels[mask]\n",
    "\n",
    "    if len(np.unique(filtered_labels)) < 2:\n",
    "        print(\"Not enough clusters to evaluate.\")\n",
    "        return None, None\n",
    "\n",
    "    silhouette = silhouette_score(filtered_features, filtered_labels)\n",
    "    dbi = davies_bouldin_score(filtered_features, filtered_labels)\n",
    "\n",
    "    print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "    print(f\"Davies-Bouldin Index: {dbi:.3f}\")\n",
    "    \n",
    "    return silhouette, dbi\n",
    "\n",
    "silhouette, dbi = evaluate_clustering_quality(\n",
    "    features=normalized_pca_components,\n",
    "    labels=data_with_dpgmm['Cluster'].values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBscan clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dbscan_clustering(normalized_pca_components, df, eps, min_samples):\n",
    "#     \"\"\"\n",
    "#     Description: Perform DBSCAN clustering on the normalized PCA components and visualize the clusters.\n",
    "#     Args:\n",
    "#         normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "#         df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "#         n_clusters (int): The number of clusters to create.\n",
    "\n",
    "#     Returns:\n",
    "#         data_with_dbscan (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "#     \"\"\"\n",
    "   \n",
    "#     # Fit the DBSCAN model\n",
    "#     dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#     clusters = dbscan.fit_predict(normalized_pca_components)\n",
    "\n",
    "#     # Add cluster labels to your original data (without overwriting)\n",
    "#     data_with_dbscan = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "#     # Insert the clusters as the second column (at index 1)\n",
    "#     data_with_dbscan.insert(1, 'Cluster', clusters)\n",
    "\n",
    "#     # Count the number of data points assigned to each cluster\n",
    "#     cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "#     # Plot the clusters\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     scatter = sns.scatterplot(x=normalized_pca_components[:, 1], y=normalized_pca_components[:, 2], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "#     # Create custom labels for the legend with the cluster counts\n",
    "#     legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "#     handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "#     # Set the custom labels in the legend\n",
    "#     plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "#     # Label the axes\n",
    "#     plt.xlabel(\"Principal Component 1\")\n",
    "#     plt.ylabel(\"Principal Component 2\")\n",
    "#     plt.title(\"PCA + DBSCAN Clustering\")\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "#     # Show the updated DataFrame with the Cluster column as the second column\n",
    "#     return data_with_dbscan\n",
    "\n",
    "# n_components = 10\n",
    "# normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "# data_with_dbscan = dbscan_clustering(normalized_pca_components, df, eps=0.5, min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_to_keep = ['all'] # 'all' or a list of cluster indices\n",
    "# plot_cluster_mean_and_std(data_with_dbscan, clusters_to_keep, 'DBSCAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'Timestamp' is a column with string dates, we convert it to datetime format.\n",
    "# data_with_dbscan['Timestamp'] = pd.to_datetime(data_with_dbscan['Timestamp'])\n",
    "\n",
    "# # Create the Plotly figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Add a scatter plot (you can choose 'line' or 'scatter' depending on the style you want)\n",
    "# fig.add_trace(go.Scatter(x=data_with_dbscan['Timestamp'], \n",
    "#                          y=data_with_dbscan['Cluster'], \n",
    "#                          mode='markers+lines',  # markers and lines\n",
    "#                          marker=dict(size=8, \n",
    "#                                      color=data_with_dbscan['Cluster'],  # Color by cluster value\n",
    "#                                      colorscale='Viridis',  # You can change the colorscale here\n",
    "#                                      colorbar=dict(title='Cluster')),  # Add a color bar to show the scale\n",
    "#                          line=dict(width=1, color='grey')))  # Customizing line color and width\n",
    "\n",
    "# # Update layout with title and labels\n",
    "# fig.update_layout(\n",
    "#     title='Assignment to Clusters Over Time with DBSCAN Clustering',\n",
    "#     xaxis_title='Time',\n",
    "#     yaxis_title='Cluster',\n",
    "#     xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "#     yaxis=dict(tickmode='linear', tick0=0, dtick=1)  # Set y-tick step size to 1\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_venn = pd.DataFrame()\n",
    "# data_venn['KMeans'] = data_with_KMeans['Cluster']\n",
    "# data_venn['GMM'] = data_with_gmm['Cluster']\n",
    "# data_venn['DBSCAN'] = data_with_dbscan['Cluster']\n",
    "\n",
    "# # Find the timestamps where all three clustering methods assigned the same cluster\n",
    "# # Find the rows where all three columns have the same value\n",
    "# set1 = data_venn[data_venn['KMeans'] == data_venn['GMM']][data_venn['KMeans'] == data_venn['DBSCAN']].index\n",
    "# set2 = data_venn[data_venn['KMeans'] == data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']].index\n",
    "# set3 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] == data_venn['DBSCAN']].index\n",
    "# set4 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']][data_venn['GMM'] == data_venn['DBSCAN']].index\n",
    "# set5 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']][data_venn['GMM'] != data_venn['DBSCAN']].index\n",
    "\n",
    "# count_set1 = len(set1)\n",
    "# count_set2 = len(set2)\n",
    "# count_set3 = len(set3)\n",
    "# count_set4 = len(set4)\n",
    "# count_set5 = len(set5)\n",
    "\n",
    "# print(\"Number of timestamps where all three clustering methods assigned the same cluster: \", count_set1)\n",
    "# print(\"Number of timestamps where KMeans and GMM assigned the same cluster but DBSCAN did not: \", count_set2)\n",
    "# print(\"Number of timestamps where KMeans and DBSCAN assigned the same cluster but GMM did not: \", count_set3)\n",
    "# print(\"Number of timestamps where GMM and DBSCAN assigned the same cluster but KMeans did not: \", count_set4)\n",
    "# print(\"Number of timestamps where all three clustering methods assigned different clusters: \", count_set5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_clusters(normalized_pca_components, n_clusters) -> None:\n",
    "#     \"\"\"\n",
    "#     Description: Evaluate different clustering algorithms (KMeans, GMM, DBSCAN) using Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "#     Args:\n",
    "#         normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "#         n_clusters (int): The maximum number of clusters to evaluate.\n",
    "\n",
    "#     Returns:\n",
    "#         None    \n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize lists to store inertia and Davies-Bouldin index for KMeans\n",
    "#     silhouette_kmeans = []  # Sum of squared distances of samples to their closest cluster center\n",
    "#     db_index_kmeans = []  # Davies-Bouldin Index\n",
    "\n",
    "#     # Initialize lists to store inertia and Davies-Bouldin index for GMM\n",
    "#     silhouette_gmm = []  \n",
    "#     db_index_gmm = []  \n",
    "\n",
    "#     # Initialize lists to store inertia and Davies-Bouldin index for DBSCAN\n",
    "#     silhouette_dbscan = []  \n",
    "#     db_index_dbscan = []  \n",
    " \n",
    "\n",
    "#     # Loop over different number of clusters\n",
    "#     for n in range(2, n_clusters + 1):\n",
    "\n",
    "#         # KMeans Clustering\n",
    "#         kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "#         kmeans_labels = kmeans.fit_predict(normalized_pca_components)\n",
    "\n",
    "#         # Davies-Bouldin Index for KMeans\n",
    "#         db_score_kmeans = davies_bouldin_score(normalized_pca_components, kmeans_labels)\n",
    "#         db_index_kmeans.append(db_score_kmeans)\n",
    "#         # Silhouette Score for KMeans\n",
    "#         silhouette_kmeans.append(silhouette_score(normalized_pca_components, kmeans_labels))\n",
    "        \n",
    "#         print(f\"K-Means - Number of clusters: {n}, Davies-Bouldin Index: {db_score_kmeans}, Silhouette Score: {silhouette_score(normalized_pca_components, kmeans_labels)}\")\n",
    "\n",
    "#         # GMM Clustering\n",
    "#         gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "#         gmm_labels = gmm.fit_predict(normalized_pca_components)\n",
    "\n",
    "#         # Davies-Bouldin Index for GMM\n",
    "#         db_score_gmm = davies_bouldin_score(normalized_pca_components, gmm_labels)\n",
    "#         db_index_gmm.append(db_score_gmm)\n",
    "#         # Silhouette Score for GMM\n",
    "#         silhouette_gmm.append(silhouette_score(normalized_pca_components, gmm_labels))\n",
    "        \n",
    "#         print(f\"GMM - Number of components: {n}, Davies-Bouldin Index: {db_score_gmm}, Silhouette Score: {silhouette_score(normalized_pca_components, gmm_labels)}\")\n",
    "\n",
    "#         # DBSCAN Clustering (DBSCAN does not require number of clusters, so we use eps and min_samples)\n",
    "#         dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "#         dbscan_labels = dbscan.fit_predict(normalized_pca_components)\n",
    "\n",
    "#         # Davies-Bouldin Index for DBSCAN (ignoring -1 labels as noise points)\n",
    "#         db_score_dbscan = davies_bouldin_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "#         db_index_dbscan.append(db_score_dbscan)\n",
    "        \n",
    "#         # Silhouette Score for DBSCAN (ignoring -1 labels as noise points)\n",
    "#         silhouette_dbscan.append(silhouette_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1]))\n",
    "        \n",
    "#         print(f\"DBSCAN - Davies-Bouldin Index: {db_score_dbscan}, Silhouette Score: {silhouette_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])}\")\n",
    "\n",
    "#     # Plot the Elbow graph and Davies-Bouldin Index in subplots\n",
    "#     fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "#     # Elbow Method Plot (Inertia)\n",
    "#     axes[0].plot(range(2, n_clusters+1), silhouette_kmeans, marker='o')\n",
    "#     axes[0].plot(range(2, n_clusters+1), silhouette_gmm, marker='o')\n",
    "#     # axes[0].plot(range(2, n_clusters+1), silhouette_dbscan, marker='o')\n",
    "#     axes[0].set_title(\"Silhouette Score for Different Number of Clusters\")\n",
    "#     axes[0].set_xlabel(\"Number of Clusters\")\n",
    "#     axes[0].set_ylabel(\"Silhouette Score\")\n",
    "#     axes[0].legend(['KMeans', 'GMM', 'DBSCAN'])\n",
    "\n",
    "#     # Davies-Bouldin Index Plot\n",
    "#     axes[1].plot(range(2, n_clusters+1), db_index_kmeans, marker='o')\n",
    "#     axes[1].plot(range(2, n_clusters+1), db_index_gmm, marker='o')\n",
    "#     # axes[1].plot(range(2, n_clusters+1), db_index_dbscan, marker='o')\n",
    "#     axes[1].set_title(\"Davies-Bouldin Index for Different Number of Clusters\")\n",
    "#     axes[1].set_xlabel(\"Number of Clusters\")\n",
    "#     axes[1].set_ylabel(\"Davies-Bouldin Index\")\n",
    "#     axes[1].legend(['KMeans', 'GMM','DBSCAN'])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the function with the normalized PCA components\n",
    "# n_components = 8\n",
    "# normalized_pca_components, df_pca = do_pca(n_components, normalized_pca_components)\n",
    "\n",
    "# n_clusters = 10\n",
    "# # evaluate_clusters(normalized_pca_components, n_clusters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
