{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Add the root project directory to the Python path\n",
    "project_root = Path.cwd().parent  # This will get the project root since the notebook is in 'notebooks/'\n",
    "sys.path.append(str(project_root))\n",
    "from configs.path_config import OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Description: Load data from a csv file containing strain distributions.\n",
    "\n",
    "    Args:\n",
    "        path (path object): The path to the csv file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.isna().sum().sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "# path = OUTPUT_DIR / 'strain_distributions' / 'N-B_Far_Comp_20091129120000_20210611160000_strain_distribution.csv'\n",
    "path = OUTPUT_DIR / 'strain_distributions' / 'N-F_Mid_Comp_20091129120000_20210611160000_strain_distribution.csv'\n",
    "df = load_data(path)\n",
    "\n",
    "df = df.iloc[0:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, threshold=1):\n",
    "    \"\"\"\n",
    "    Description: Remove outliers from the data in two ways:\n",
    "        1. Remove rows where the mean exceeds the threshold deviation from the overall mean based on absolute values.\n",
    "        2. Remove rows where any value in the row exceeds the threshold based on its own mean (using absolute values).\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "        threshold (float): The threshold for determining outliers based on the mean. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "        df (pd DataFrame): The data cleaned of outliers, with the timestamp column.\n",
    "    \"\"\"\n",
    "    df_strain = df.drop(columns='Timestamp')\n",
    "    \n",
    "    # 1. Remove rows where the mean is above the threshold deviation from the overall mean (based on absolute values)\n",
    "    abs_df = df_strain.abs()  # Take the absolute values of the dataframe\n",
    "    means = abs_df.mean(axis=1)  # Calculate mean of absolute values for each row\n",
    "    mean_val = means.mean()  # Mean of all row means\n",
    "    std_val = means.std()  # Standard deviation of the row means\n",
    "\n",
    "    # Define a threshold for the mean-based outliers (absolute values)\n",
    "    mean_threshold = threshold\n",
    "\n",
    "    # Find outliers based on absolute mean deviation\n",
    "    mean_outliers = means[np.abs(means - mean_val) > mean_threshold * std_val]\n",
    "    print(\"Mean-based outliers (absolute values):\")\n",
    "    print(mean_outliers)\n",
    "\n",
    "    # 2. Remove rows where any value exceeds the threshold based on the row's mean (using absolute values)\n",
    "    row_outliers = []\n",
    "    for idx, row in abs_df.iterrows():\n",
    "        row_mean = row.mean()  # Mean of the absolute values in the row\n",
    "        row_std = row.std()  # Standard deviation of the absolute values in the row\n",
    "        threshold_value = row_mean + threshold * row_std  # Define threshold for each row based on absolute values\n",
    "        \n",
    "        # Check if any value in the row exceeds the calculated threshold (absolute value)\n",
    "        if (row > threshold_value).any():\n",
    "            row_outliers.append(idx)  # Keep track of the outlier row indices\n",
    "    \n",
    "    print(\"Outliers based on row threshold (absolute values):\")\n",
    "    print(df_strain.loc[row_outliers])\n",
    "\n",
    "    # Combine both outliers (mean-based and row-based) and drop them\n",
    "    outliers = mean_outliers.index.union(row_outliers)\n",
    "    \n",
    "    # Remove the rows from the data\n",
    "    df_strain = df_strain.drop(outliers)  # Removed outliers without timestamp\n",
    "    df = df.drop(outliers)  # Removed outliers with timestamp\n",
    "    \n",
    "    print(f\"Total number of outliers removed: {len(outliers)}\")\n",
    "\n",
    "    return df_strain, df\n",
    "\n",
    "# Usage\n",
    "df_strain, df = remove_outliers(df, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, df_strain):\n",
    "    \"\"\"\n",
    "    Description: Perform PCA on the data.\n",
    "\n",
    "    Args:\n",
    "        n_components (int): The number of principal components to keep.\n",
    "        df_strain (pd DataFrame): The data.\n",
    "\n",
    "    Returns:\n",
    "        df_pca (pd DataFrame): The principal components for each timestamp.\n",
    "        normalized_pca_components (np array) : The normalized principal components.\n",
    "    \"\"\"\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    pca.fit(df_strain)\n",
    "\n",
    "    # Apply PCA to the entire strain data (matrix-wise)\n",
    "    pca_results = pca.transform(df_strain)\n",
    "\n",
    "    # Normalize the results\n",
    "    normalized_pca_components = StandardScaler().fit_transform(pca_results)\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df_pca = pd.DataFrame(normalized_pca_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Add timestamps back\n",
    "    df_pca.insert(0, 'Timestamp', df['Timestamp'].values)\n",
    "\n",
    "    return normalized_pca_components, df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class DP_FGS:\n",
    "    def __init__(self, a, l0, R0, j0, m0, omax, max_components=10):\n",
    "        \"\"\"\n",
    "        Initialize the parameters for the Dirichlet Process Gibbs Sampler (DP-FGS).\n",
    "        \n",
    "        Args:\n",
    "            a: Concentration parameter for the Dirichlet Process.\n",
    "            l0: Hyperparameter for the Gaussian mean (prior mean).\n",
    "            R0: Hyperparameter for the Gaussian covariance (prior covariance).\n",
    "            j0: Hyperparameter for the cluster size.\n",
    "            m0: Hyperparameter for the new cluster's mean.\n",
    "            omax: Maximum number of recent data points considered for updating.\n",
    "            max_components: Maximum number of clusters to consider.\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.l0 = l0\n",
    "        self.R0 = R0\n",
    "        self.j0 = j0\n",
    "        self.m0 = m0\n",
    "        self.omax = omax\n",
    "        self.max_components = max_components\n",
    "        \n",
    "        self.cluster_params = []\n",
    "        self.cluster_labels = []\n",
    "        self.data_points = []\n",
    "        \n",
    "    def _initialize_cluster_params(self, data_point):\n",
    "        \"\"\"\n",
    "        Initialize cluster parameters for a new cluster.\n",
    "        \n",
    "        Args:\n",
    "            data_point: New data point.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Initialized mean and covariance for the new cluster.\n",
    "        \"\"\"\n",
    "        mean = np.random.normal(self.l0, self.m0)\n",
    "        covariance = self.R0 * np.eye(len(data_point))  # Assuming diagonal covariance matrix\n",
    "        return mean, covariance\n",
    "        \n",
    "    def _predict_cluster(self, data_point, cluster_id):\n",
    "        \"\"\"\n",
    "        Predict the likelihood of the data point for a given cluster using a Gaussian distribution.\n",
    "        \n",
    "        Args:\n",
    "            data_point: New data point.\n",
    "            cluster_id: Cluster index to evaluate.\n",
    "        \n",
    "        Returns:\n",
    "            float: Log-likelihood of the data point under the Gaussian distribution of the cluster.\n",
    "        \"\"\"\n",
    "        mean, covariance = self.cluster_params[cluster_id]\n",
    "        \n",
    "        # Ensure data_point is a 1D array\n",
    "        data_point = np.squeeze(data_point)  # Flatten the data point if it's not already a 1D array\n",
    "        \n",
    "        if mean.ndim != 1:\n",
    "            raise ValueError(f\"Mean should be a 1D array, but got shape {mean.shape}\")\n",
    "        if data_point.ndim != 1:\n",
    "            raise ValueError(f\"Data point should be a 1D array, but got shape {data_point.shape}\")\n",
    "        \n",
    "        # Now compute the log-likelihood\n",
    "        return multivariate_normal.logpdf(data_point, mean=mean, cov=covariance)\n",
    "        \n",
    "    def _sample_cluster_assignment(self, data_point):\n",
    "        \"\"\"\n",
    "        Sample the cluster assignment for the new data point based on the predictive posterior.\n",
    "        \n",
    "        Args:\n",
    "            data_point: New data point.\n",
    "        \n",
    "        Returns:\n",
    "            int: Cluster index that the data point is assigned to.\n",
    "        \"\"\"\n",
    "        log_likelihoods = []\n",
    "        for cluster_id in range(len(self.cluster_params)):\n",
    "            log_likelihoods.append(self._predict_cluster(data_point, cluster_id))\n",
    "        \n",
    "        # For new cluster\n",
    "        log_likelihoods.append(np.log(self.a))  # Prior for a new cluster\n",
    "        \n",
    "        # Compute normalized probabilities (using softmax)\n",
    "        log_probs = np.array(log_likelihoods) - np.max(log_likelihoods)  # Avoid numerical issues\n",
    "        probs = np.exp(log_probs) / np.sum(np.exp(log_probs))\n",
    "        \n",
    "        return np.random.choice(len(self.cluster_params) + 1, p=probs)\n",
    "        \n",
    "    def _update_cluster_params(self, cluster_id, data_point):\n",
    "        \"\"\"\n",
    "        Update the parameters (mean and covariance) of a cluster based on the assigned data point.\n",
    "        \n",
    "        Args:\n",
    "            cluster_id: The cluster to update.\n",
    "            data_point: New data point assigned to this cluster.\n",
    "        \"\"\"\n",
    "        # Use conjugate priors to update mean and covariance (simplified version)\n",
    "        mean, covariance = self.cluster_params[cluster_id]\n",
    "        \n",
    "        # Here, you would implement the actual parameter update using the data and the prior.\n",
    "        # This is a placeholder, assuming updates based on a simple Gaussian update rule.\n",
    "        \n",
    "        updated_mean = np.mean(data_point)  # Update with new data\n",
    "        updated_cov = covariance + np.var(data_point)  # Placeholder update rule\n",
    "        \n",
    "        self.cluster_params[cluster_id] = (updated_mean, updated_cov)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Perform dynamic clustering on the data using the Gibbs sampler.\n",
    "        \n",
    "        Args:\n",
    "            data: The dataset (2D array) where each row is a data point to be clustered.\n",
    "        \"\"\"\n",
    "        N = 0  # Number of observed points\n",
    "        for i, data_point in enumerate(data):\n",
    "            N += 1\n",
    "            if N > self.omax:\n",
    "                o = self.omax\n",
    "            else:\n",
    "                o = N  # Number of points to consider\n",
    "            \n",
    "            # Random permutation of the last 'o' data points\n",
    "            recent_data_points = self.data_points[-o:]\n",
    "            np.random.shuffle(recent_data_points)\n",
    "            \n",
    "            # Sample a cluster assignment for the new point\n",
    "            cluster_id = self._sample_cluster_assignment(data_point)\n",
    "            \n",
    "            if cluster_id == len(self.cluster_params):  # New cluster\n",
    "                mean, covariance = self._initialize_cluster_params(data_point)\n",
    "                self.cluster_params.append((mean, covariance))\n",
    "            \n",
    "            # Add the point to the assigned cluster\n",
    "            self.cluster_labels.append(cluster_id)\n",
    "            self.data_points.append(data_point)\n",
    "            \n",
    "            # Update the cluster parameters\n",
    "            self._update_cluster_params(cluster_id, data_point)\n",
    "    \n",
    "    def get_cluster_assignments(self):\n",
    "        \"\"\"Get the cluster assignments for all data points.\"\"\"\n",
    "        return self.cluster_labels\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# Initialize the DP-FGS model\n",
    "dp_fgs = DP_FGS(a=1.0, l0=0.0, R0=1.0, j0=1.0, m0=0.0, omax=10)\n",
    "\n",
    "# Example data points (replace with actual SHM data)\n",
    "data = np.random.randn(100, 2)  # 100 data points, 2 features\n",
    "\n",
    "# Perform dynamic clustering\n",
    "dp_fgs.fit(data)\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = dp_fgs.get_cluster_assignments()\n",
    "\n",
    "# Print cluster assignments for each data point\n",
    "print(cluster_assignments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_clustering_dpgmm(data, max_components=10, init_points=10):\n",
    "    \"\"\"\n",
    "    Perform dynamic clustering on the data, adding one data point at a time, using DPGMM.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The dataset to cluster (PCA components).\n",
    "                          This should be a 2D numpy array where each row represents \n",
    "                          the PCA components of a strain distribution at a particular timestamp.\n",
    "                          Shape: (n_samples, n_features)\n",
    "        max_components (int): Maximum number of components for Bayesian GMM.\n",
    "        init_points (int): Number of initial points to use for model initialization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the original PCA components and their corresponding cluster labels.\n",
    "                      Shape: (n_samples, n_features + 1) where the last column is 'Cluster'.\n",
    "    \"\"\"\n",
    "    # Initialize the model with the maximum components and full covariance\n",
    "    model = BayesianGaussianMixture(n_components=max_components, covariance_type='full', random_state=42)\n",
    "    \n",
    "    # Initialize the list to store cluster labels for each point\n",
    "    cluster_labels = []\n",
    "    \n",
    "    # First fit the model with the first 'init_points' points\n",
    "    initial_data = data[:init_points]\n",
    "    \n",
    "    # Instead of predicting the clusters, manually assign cluster 0 to all initial points\n",
    "    cluster_labels.extend([0] * init_points)\n",
    "    \n",
    "    # Fit the model with the initial data points\n",
    "    model.fit(initial_data)\n",
    "    \n",
    "    # For the first 'init_points', assign clusters\n",
    "    # cluster_labels.extend(model.predict(initial_data))  # Predict cluster labels for initial points\n",
    "    \n",
    "    # Then, fit and predict incrementally for the remaining points\n",
    "    for i in range(init_points, len(data)):\n",
    "        point = data[i].reshape(1, -1)  # Reshape the data to be a 2D array (one row)\n",
    "\n",
    "        # Fit the model with data up to the current point (i+1)\n",
    "        model.fit(data[:i+1])  # Incrementally fit the model with all data up to this point\n",
    "        \n",
    "        # Predict the cluster for the current point\n",
    "        cluster_label = model.predict(point)\n",
    "        cluster_labels.append(cluster_label[0])\n",
    "        # print(f'Cluster labels: \\n{cluster_labels}')\n",
    "    \n",
    "    # Create a DataFrame with the original PCA components and cluster labels\n",
    "    data_with_dpgmm = pd.DataFrame(data, columns=[f'PC{i+1}' for i in range(data.shape[1])])\n",
    "    data_with_dpgmm['Cluster'] = cluster_labels  # Add the cluster labels as a new column\n",
    "    \n",
    "    return data_with_dpgmm\n",
    "\n",
    "def plot_dynamic_clustering(data_with_dpgmm):\n",
    "    \"\"\"\n",
    "    Plot the results of dynamic clustering over time, showing how cluster assignments evolve.\n",
    "    \n",
    "    Args:\n",
    "        data_with_dpgmm (pd.DataFrame): DataFrame containing PCA components and their corresponding cluster labels.\n",
    "                                         It should have columns 'PC1', 'PC2', ..., 'Cluster'.\n",
    "    \"\"\"\n",
    "    # Extract the first two principal components for visualization\n",
    "    pc1 = data_with_dpgmm['PC1']\n",
    "    pc2 = data_with_dpgmm['PC2']\n",
    "    \n",
    "    # Create a scatter plot where the color represents the cluster label\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(pc1, pc2, c=data_with_dpgmm['Cluster'], cmap='viridis', s=50)\n",
    "    \n",
    "    # Add color bar to indicate the cluster labels\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Dynamic Clustering of PCA Components')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "n_components = 10\n",
    "# df_strain should have shape (n_samples, n_features), where n_samples is the number of timestamps, \n",
    "# and n_features is the number of features (or sensors).\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)  # Perform PCA\n",
    "\n",
    "# Perform dynamic clustering using DPGMM\n",
    "data_with_dpgmm = dynamic_clustering_dpgmm(normalized_pca_components, max_components=10, init_points=10)\n",
    "plot_dynamic_clustering(data_with_dpgmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dpgmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpgmm_clustering(normalized_pca_components, df):\n",
    "\n",
    "    bgm = BayesianGaussianMixture(n_components=10, covariance_type='full', random_state=42)\n",
    "    bgm.fit(normalized_pca_components)\n",
    "    clusters = bgm.predict(normalized_pca_components)\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_dpgmm = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_dpgmm.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    n_clusters = len(data_with_dpgmm['Cluster'].unique())\n",
    "\n",
    "    # data_with_dpgmm.insert(2, 'Assigned_Cluster_Prob', assigned_prob)  # Insert probability for the assigned cluster\n",
    "\n",
    "\n",
    "    # Count the number of data points assigned to each cluster\n",
    "    cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Create custom labels for the legend with the cluster counts\n",
    "    legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Set the custom labels in the legend\n",
    "    plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + dpgmm Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_dpgmm\n",
    "\n",
    "n_components = 10\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "data_with_dpgmm = dpgmm_clustering(normalized_pca_components, df)\n",
    "# data_with_dpgmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_over_time(data_with_clusters, method) -> None:\n",
    "    \"\"\"\n",
    "    Description: Plot the assignment to clusters over time with any clustering method.\n",
    "\n",
    "    Args:\n",
    "        data_with_KMeans (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "        metod (str): The clustering method used (e.g., 'KMeans', 'dpgmm', 'DBSCAN'). This will be used in the title.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Assuming 'Timestamp' is a column with string dates, we convert it to datetime format.\n",
    "    data_with_clusters['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "    # Create the Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add a scatter plot (you can choose 'line' or 'scatter' depending on the style you want)\n",
    "    fig.add_trace(go.Scatter(x=data_with_clusters['Timestamp'], \n",
    "                            y=data_with_clusters['Cluster'], \n",
    "                            mode='markers+lines',  # markers and lines\n",
    "                            marker=dict(size=8, \n",
    "                                        color=data_with_clusters['Cluster'],  # Color by cluster value\n",
    "                                        colorscale='Viridis',  # You can change the colorscale here\n",
    "                                        colorbar=dict(title='Cluster')),  # Add a color bar to show the scale\n",
    "                            line=dict(width=1, color='grey')))  # Customizing line color and width\n",
    "\n",
    "    # Update layout with title and labels\n",
    "    fig.update_layout(\n",
    "        title=f'Assignment to Clusters Over Time with {method} Clustering',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Cluster',\n",
    "        xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "        yaxis=dict(tickmode='linear', tick0=0, dtick=1)  # Set y-tick step size to 1\n",
    "    )\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_clusters_over_time(data_with_dpgmm, 'DPGMM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
