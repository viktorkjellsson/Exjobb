{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "# Add the root project directory to the Python path\n",
    "project_root = Path.cwd().parent  # This will get the project root since the notebook is in 'notebooks/'\n",
    "sys.path.append(str(project_root))\n",
    "from configs.path_config import OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Description: Load data from a csv file containing strain distributions.\n",
    "\n",
    "    Args:\n",
    "        path (path object): The path to the csv file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.isna().sum().sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "# path = OUTPUT_DIR / 'strain_distributions' / 'N-B_Far_Comp_20091129120000_20210611160000_strain_distribution.csv'\n",
    "path = OUTPUT_DIR / 'strain_distributions' / 'N-F_Mid_Comp_20091129120000_20210611160000_strain_distribution.csv'\n",
    "df = load_data(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, threshold=1):\n",
    "    \"\"\"\n",
    "    Description: Remove outliers from the data in two ways:\n",
    "        1. Remove rows where the mean exceeds the threshold deviation from the overall mean based on absolute values.\n",
    "        2. Remove rows where any value in the row exceeds the threshold based on its own mean (using absolute values).\n",
    "\n",
    "    Args:\n",
    "        df (pd DataFrame): The data loaded from the csv file.\n",
    "        threshold (float): The threshold for determining outliers based on the mean. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "        df (pd DataFrame): The data cleaned of outliers, with the timestamp column.\n",
    "    \"\"\"\n",
    "    df_strain = df.drop(columns='Timestamp')\n",
    "    \n",
    "    # 1. Remove rows where the mean is above the threshold deviation from the overall mean (based on absolute values)\n",
    "    abs_df = df_strain.abs()  # Take the absolute values of the dataframe\n",
    "    means = abs_df.mean(axis=1)  # Calculate mean of absolute values for each row\n",
    "    mean_val = means.mean()  # Mean of all row means\n",
    "    std_val = means.std()  # Standard deviation of the row means\n",
    "\n",
    "    # Define a threshold for the mean-based outliers (absolute values)\n",
    "    mean_threshold = threshold\n",
    "\n",
    "    # Find outliers based on absolute mean deviation\n",
    "    mean_outliers = means[np.abs(means - mean_val) > mean_threshold * std_val]\n",
    "    print(\"Mean-based outliers (absolute values):\")\n",
    "    print(mean_outliers)\n",
    "\n",
    "    # 2. Remove rows where any value exceeds the threshold based on the row's mean (using absolute values)\n",
    "    row_outliers = []\n",
    "    for idx, row in abs_df.iterrows():\n",
    "        row_mean = row.mean()  # Mean of the absolute values in the row\n",
    "        row_std = row.std()  # Standard deviation of the absolute values in the row\n",
    "        threshold_value = row_mean + threshold * row_std  # Define threshold for each row based on absolute values\n",
    "        \n",
    "        # Check if any value in the row exceeds the calculated threshold (absolute value)\n",
    "        if (row > threshold_value).any():\n",
    "            row_outliers.append(idx)  # Keep track of the outlier row indices\n",
    "    \n",
    "    print(\"Outliers based on row threshold (absolute values):\")\n",
    "    print(df_strain.loc[row_outliers])\n",
    "\n",
    "    # Combine both outliers (mean-based and row-based) and drop them\n",
    "    outliers = mean_outliers.index.union(row_outliers)\n",
    "    \n",
    "    # Remove the rows from the data\n",
    "    df_strain = df_strain.drop(outliers)  # Removed outliers without timestamp\n",
    "    df = df.drop(outliers)  # Removed outliers with timestamp\n",
    "    \n",
    "    print(f\"Total number of outliers removed: {len(outliers)}\")\n",
    "\n",
    "    return df_strain, df\n",
    "\n",
    "# Usage\n",
    "df_strain, df = remove_outliers(df, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_outliers(df):\n",
    "#     \"\"\"\n",
    "#     Description: Remove outliers from the data by calculating the mean of each row and removing rows where the mean deviates\n",
    "#     from the overall mean.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd DataFrame): The data loaded from the csv file.\n",
    "\n",
    "#     Returns:\n",
    "#         df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "#         df (pd DataFrame): The data cleaned of outliers, with the timestamp column.\n",
    "#     \"\"\"\n",
    "#     df_strain = df.drop(columns = 'Timestamp')\n",
    "#     means = df_strain.mean(axis=1)  # Calculate mean for each row\n",
    "\n",
    "#     # Calculate the mean and standard deviation of the row means\n",
    "#     mean_val = means.mean()\n",
    "#     std_val = means.std()\n",
    "\n",
    "#     # Define a threshold for outliers (3 standard deviations from the mean)\n",
    "#     threshold = 1\n",
    "\n",
    "#     # Find outliers: rows where the absolute deviation from the mean is greater than the threshold\n",
    "#     outliers = means[np.abs(means - mean_val) > threshold * std_val]\n",
    "#     print(\"Outliers:\")\n",
    "#     print(outliers)\n",
    "\n",
    "#     df_strain = df_strain.drop(outliers.index) #Removed outliers without timestamp\n",
    "#     df = df.drop(outliers.index) #Removed outliers with timestamp\n",
    "#     print(\"Number of outliers removed: \", len(outliers))\n",
    "\n",
    "#     return df_strain, df\n",
    "\n",
    "# df_strain, df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "# Add the root project directory to the Python path\n",
    "project_root = Path.cwd().parent  # This will get the project root since the notebook is in 'notebooks/'\n",
    "sys.path.append(str(project_root))\n",
    "from configs.path_config import OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform PCA on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_data(df_strain):\n",
    "#     \"\"\"\n",
    "#     Description: Scale the data using MinMaxScaler.\n",
    "\n",
    "#     Args:\n",
    "#         df_strain (pd DataFrame): The data cleaned of outliers, without the timestamp column.\n",
    "\n",
    "#     Returns:\n",
    "#         df_strain (pd DataFrame): The scaled data.\n",
    "#     \"\"\"\n",
    "#     # Exclude timestamp in column 0\n",
    "#     df_strain = df_strain.iloc[:, 1:]\n",
    "\n",
    "#     # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaler = StandardScaler()\n",
    "#     df_strain = pd.DataFrame(scaler.fit_transform(df_strain.T).T, columns=df_strain.columns)\n",
    "\n",
    "#     return df_strain\n",
    "\n",
    "# df_strain = scale_data(df_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_variance(df_strain) -> None:  \n",
    "    \"\"\"\n",
    "    Descripition: Plots the explained variance by number of components for PCA.\n",
    "\n",
    "    Args:\n",
    "        df_strain (pd DataFrame): The data.\n",
    "\n",
    "    Returns:\n",
    "        None    \n",
    "    \"\"\"\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    # Set the number of components directly (e.g., 5 components)\n",
    "    pca = PCA(n_components=10)\n",
    "    pca.fit(df_strain)\n",
    "    # \n",
    "    # Get the explained variance ratio\n",
    "    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "    # Plot the cumulative explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(per_var) + 1), per_var.cumsum(), marker=\"o\", linestyle=\"--\")\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Percentage Cumulative of Explained Variance\")\n",
    "    plt.xlabel(\"Number of Principal Components\")\n",
    "    plt.xticks(range(1, len(per_var) + 1, 1))\n",
    "    plt.title(\"Explained Variance by Number of Components\")\n",
    "    plt.show()\n",
    "\n",
    "explain_variance(df_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, df_strain):\n",
    "    \"\"\"\n",
    "    Description: Perform PCA on the data.\n",
    "\n",
    "    Args:\n",
    "        n_components (int): The number of principal components to keep.\n",
    "        df_strain (pd DataFrame): The data.\n",
    "\n",
    "    Returns:\n",
    "        df_pca (pd DataFrame): The principal components for each timestamp.\n",
    "        normalized_pca_components (np array) : The normalized principal components.\n",
    "    \"\"\"\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    pca.fit(df_strain)\n",
    "\n",
    "    # Apply PCA to the entire strain data (matrix-wise)\n",
    "    pca_results = pca.transform(df_strain)\n",
    "\n",
    "    # Normalize the results\n",
    "    normalized_pca_components = StandardScaler().fit_transform(pca_results)\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df_pca = pd.DataFrame(normalized_pca_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Add timestamps back\n",
    "    df_pca.insert(0, 'Timestamp', df['Timestamp'].values)\n",
    "\n",
    "    return normalized_pca_components, df_pca\n",
    "\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(normalized_pca_components, df, n_clusters):\n",
    "    \"\"\"\n",
    "    Description: Perform K-Means clustering on the normalized PCA components and visualize the clusters.\n",
    "    Args:\n",
    "        normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "        df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "        n_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        data_with_KMeans (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "    \"\"\"\n",
    "   \n",
    "    kmeans = KMeans(n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(normalized_pca_components)\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_KMeans = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_KMeans.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    # Count the number of data points assigned to each cluster\n",
    "    cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Create custom labels for the legend with the cluster counts\n",
    "    legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Set the custom labels in the legend\n",
    "    plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + KMeans Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_KMeans\n",
    "\n",
    "n_components = 10\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "n_clusters = 6\n",
    "data_with_KMeans = kmeans_clustering(normalized_pca_components, df, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the cluster assignment over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_over_time(data_with_clusters, method) -> None:\n",
    "    \"\"\"\n",
    "    Description: Plot the assignment to clusters over time with any clustering method.\n",
    "\n",
    "    Args:\n",
    "        data_with_KMeans (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "        metod (str): The clustering method used (e.g., 'KMeans', 'GMM', 'DBSCAN'). This will be used in the title.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Assuming 'Timestamp' is a column with string dates, we convert it to datetime format.\n",
    "    data_with_clusters['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "    # Create the Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add a scatter plot (you can choose 'line' or 'scatter' depending on the style you want)\n",
    "    fig.add_trace(go.Scatter(x=data_with_clusters['Timestamp'], \n",
    "                            y=data_with_clusters['Cluster'], \n",
    "                            mode='markers+lines',  # markers and lines\n",
    "                            marker=dict(size=8, \n",
    "                                        color=data_with_clusters['Cluster'],  # Color by cluster value\n",
    "                                        colorscale='Viridis',  # You can change the colorscale here\n",
    "                                        colorbar=dict(title='Cluster')),  # Add a color bar to show the scale\n",
    "                            line=dict(width=1, color='grey')))  # Customizing line color and width\n",
    "\n",
    "    # Update layout with title and labels\n",
    "    fig.update_layout(\n",
    "        title=f'Assignment to Clusters Over Time with {method} Clustering',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Cluster',\n",
    "        xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "        yaxis=dict(tickmode='linear', tick0=0, dtick=1)  # Set y-tick step size to 1\n",
    "    )\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_clusters_over_time(data_with_KMeans, 'KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the mean distribution for each cluster and the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_mean_and_std(data_with_clusters, clusters_to_keep, method) -> None:\n",
    "    \"\"\"\n",
    "    Description: Plot the mean strain values for each cluster with uncertainty (standard deviation).\n",
    "\n",
    "    Args:\n",
    "        data_with_clusters (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels. (e.g., data_with_KMeans)\n",
    "        clusters_to_keep (list): List of which clusters to keep (e.g., [0, 1, 2]). Use ['all'] to keep all clusters.\n",
    "        method (str): The clustering method used (e.g., 'KMeans', 'GMM', 'DBSCAN'). This will be used in the title.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the user wants to keep all clusters\n",
    "    if clusters_to_keep == ['all']:\n",
    "        clusters_to_keep = data_with_clusters['Cluster'].unique()  # Use the correct attribute to get unique cluster indices\n",
    "\n",
    "    # Group the data by Cluster and compute the mean strain values for each cluster\n",
    "    df_cluster_mean = data_with_clusters.drop(columns='Timestamp').groupby('Cluster').mean()\n",
    "\n",
    "    # Convert column names (assumed to be distances along the beam) to numeric values\n",
    "    x_values = pd.to_numeric(df_cluster_mean.columns, errors='coerce')  # Convert column names to numeric\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected rows\n",
    "    df_cluster = df_cluster_mean.loc[clusters_to_keep]  # Use .loc for row selection\n",
    "\n",
    "    # Step 1: Calculate the standard deviation of strain values for each cluster\n",
    "    df_cluster_std = data_with_clusters.drop(columns='Timestamp').groupby('Cluster').std()\n",
    "\n",
    "    average_std = df_cluster_std.mean(axis=1)\n",
    "\n",
    "    # Step 2: Plot the cluster centroids with uncertainty (shaded region)\n",
    "    plt.figure(figsize=(30, 6))\n",
    "\n",
    "    # Loop over clusters and plot their mean strain and uncertainty (standard deviation)\n",
    "    for i, cluster in enumerate(df_cluster.index):\n",
    "        # Plot the mean strain distribution for the current cluster\n",
    "        plt.plot(x_values, df_cluster.loc[cluster], label=f'Cluster {cluster} - Mean, Avg. std: {average_std[i]:.2f}', linewidth=2)\n",
    "        \n",
    "        # Plot the uncertainty region (standard deviation) around the mean\n",
    "        plt.fill_between(x_values, \n",
    "                         df_cluster.loc[cluster] - df_cluster_std.loc[cluster], \n",
    "                         df_cluster.loc[cluster] + df_cluster_std.loc[cluster], \n",
    "                         alpha=0.3)  # Shaded region for uncertainty\n",
    "\n",
    "    # Customizing the plot\n",
    "    plt.xlabel('Distance [m]')\n",
    "    plt.ylabel('Strain (Mean Value)')\n",
    "    plt.title(f'{method} Clustering Centroids with Uncertainty (Standard Deviation) with [INSERT DYNNAMIC NAMING] Scaling')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "clusters_to_keep = [2] # 'all' or a list of cluster indices\n",
    "plot_cluster_mean_and_std(data_with_KMeans, clusters_to_keep, 'KMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(normalized_pca_components, df, n_clusters):\n",
    "    \"\"\"\n",
    "    Description: Perform GMM clustering on the normalized PCA components and visualize the clusters.\n",
    "    Args:\n",
    "        normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "        df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "        n_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        data_with_gmm (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "    \"\"\"\n",
    "   \n",
    "    gmm = GaussianMixture(n_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(normalized_pca_components)\n",
    "\n",
    "    # Extract cluster probabilities\n",
    "    probabilities = gmm.predict_proba(normalized_pca_components)\n",
    "\n",
    "    # Get the probability of the assigned cluster\n",
    "    assigned_prob = probabilities[np.arange(len(clusters)), clusters]\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_gmm = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_gmm.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    data_with_gmm.insert(2, 'Assigned_Cluster_Prob', assigned_prob)  # Insert probability for the assigned cluster\n",
    "\n",
    "\n",
    "    # Count the number of data points assigned to each cluster\n",
    "    cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Create custom labels for the legend with the cluster counts\n",
    "    legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Set the custom labels in the legend\n",
    "    plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + GMM Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_gmm\n",
    "\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "n_clusters = 4\n",
    "data_with_gmm = gmm_clustering(normalized_pca_components, df, n_clusters)\n",
    "data_with_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_over_time(data_with_gmm, 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_to_keep = ['all'] # 'all' or a list of cluster indices\n",
    "plot_cluster_mean_and_std(data_with_gmm, clusters_to_keep, 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(normalized_pca_components, df, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Description: Perform DBSCAN clustering on the normalized PCA components and visualize the clusters.\n",
    "    Args:\n",
    "        normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "        df (pd DataFrame): The original DataFrame including the timestamps.\n",
    "        n_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        data_with_dbscan (pd DataFrame): The original DataFrame including the timestamps with the addition of the cluster labels.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Fit the DBSCAN model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(normalized_pca_components)\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_dbscan = df.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_dbscan.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    # Count the number of data points assigned to each cluster\n",
    "    cluster_counts = {i: sum(clusters == i) for i in range(n_clusters)}\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(x=normalized_pca_components[:, 1], y=normalized_pca_components[:, 2], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Create custom labels for the legend with the cluster counts\n",
    "    legend_labels = [f'Cluster {i} (n={cluster_counts[i]})' for i in range(n_clusters)]\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Set the custom labels in the legend\n",
    "    plt.legend(handles=handles, labels=legend_labels, title='Cluster')\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + DBSCAN Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_dbscan\n",
    "\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, df_strain)\n",
    "\n",
    "data_with_dbscan = dbscan_clustering(normalized_pca_components, df, eps=0.5, min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_to_keep = ['all'] # 'all' or a list of cluster indices\n",
    "# plot_cluster_mean_and_std(data_with_dbscan, clusters_to_keep, 'DBSCAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Timestamp' is a column with string dates, we convert it to datetime format.\n",
    "data_with_dbscan['Timestamp'] = pd.to_datetime(data_with_dbscan['Timestamp'])\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a scatter plot (you can choose 'line' or 'scatter' depending on the style you want)\n",
    "fig.add_trace(go.Scatter(x=data_with_dbscan['Timestamp'], \n",
    "                         y=data_with_dbscan['Cluster'], \n",
    "                         mode='markers+lines',  # markers and lines\n",
    "                         marker=dict(size=8, \n",
    "                                     color=data_with_dbscan['Cluster'],  # Color by cluster value\n",
    "                                     colorscale='Viridis',  # You can change the colorscale here\n",
    "                                     colorbar=dict(title='Cluster')),  # Add a color bar to show the scale\n",
    "                         line=dict(width=1, color='grey')))  # Customizing line color and width\n",
    "\n",
    "# Update layout with title and labels\n",
    "fig.update_layout(\n",
    "    title='Assignment to Clusters Over Time with DBSCAN Clustering',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Cluster',\n",
    "    xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "    yaxis=dict(tickmode='linear', tick0=0, dtick=1)  # Set y-tick step size to 1\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venn = pd.DataFrame()\n",
    "data_venn['KMeans'] = data_with_KMeans['Cluster']\n",
    "data_venn['GMM'] = data_with_gmm['Cluster']\n",
    "data_venn['DBSCAN'] = data_with_dbscan['Cluster']\n",
    "\n",
    "# Find the timestamps where all three clustering methods assigned the same cluster\n",
    "# Find the rows where all three columns have the same value\n",
    "set1 = data_venn[data_venn['KMeans'] == data_venn['GMM']][data_venn['KMeans'] == data_venn['DBSCAN']].index\n",
    "set2 = data_venn[data_venn['KMeans'] == data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']].index\n",
    "set3 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] == data_venn['DBSCAN']].index\n",
    "set4 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']][data_venn['GMM'] == data_venn['DBSCAN']].index\n",
    "set5 = data_venn[data_venn['KMeans'] != data_venn['GMM']][data_venn['KMeans'] != data_venn['DBSCAN']][data_venn['GMM'] != data_venn['DBSCAN']].index\n",
    "\n",
    "count_set1 = len(set1)\n",
    "count_set2 = len(set2)\n",
    "count_set3 = len(set3)\n",
    "count_set4 = len(set4)\n",
    "count_set5 = len(set5)\n",
    "\n",
    "print(\"Number of timestamps where all three clustering methods assigned the same cluster: \", count_set1)\n",
    "print(\"Number of timestamps where KMeans and GMM assigned the same cluster but DBSCAN did not: \", count_set2)\n",
    "print(\"Number of timestamps where KMeans and DBSCAN assigned the same cluster but GMM did not: \", count_set3)\n",
    "print(\"Number of timestamps where GMM and DBSCAN assigned the same cluster but KMeans did not: \", count_set4)\n",
    "print(\"Number of timestamps where all three clustering methods assigned different clusters: \", count_set5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(normalized_pca_components, n_clusters) -> None:\n",
    "    \"\"\"\n",
    "    Description: Evaluate different clustering algorithms (KMeans, GMM, DBSCAN) using Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        normalized_pca_components (nparray): The PCA components normalized using StandardScaler.\n",
    "        n_clusters (int): The maximum number of clusters to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        None    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for KMeans\n",
    "    silhouette_kmeans = []  # Sum of squared distances of samples to their closest cluster center\n",
    "    db_index_kmeans = []  # Davies-Bouldin Index\n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for GMM\n",
    "    silhouette_gmm = []  \n",
    "    db_index_gmm = []  \n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for DBSCAN\n",
    "    silhouette_dbscan = []  \n",
    "    db_index_dbscan = []  \n",
    " \n",
    "\n",
    "    # Loop over different number of clusters\n",
    "    for n in range(2, n_clusters + 1):\n",
    "\n",
    "        # KMeans Clustering\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(normalized_pca_components)\n",
    "\n",
    "        # Davies-Bouldin Index for KMeans\n",
    "        db_score_kmeans = davies_bouldin_score(normalized_pca_components, kmeans_labels)\n",
    "        db_index_kmeans.append(db_score_kmeans)\n",
    "        # Silhouette Score for KMeans\n",
    "        silhouette_kmeans.append(silhouette_score(normalized_pca_components, kmeans_labels))\n",
    "        \n",
    "        print(f\"K-Means - Number of clusters: {n}, Davies-Bouldin Index: {db_score_kmeans}, Silhouette Score: {silhouette_score(normalized_pca_components, kmeans_labels)}\")\n",
    "\n",
    "        # GMM Clustering\n",
    "        gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "        gmm_labels = gmm.fit_predict(normalized_pca_components)\n",
    "\n",
    "        # Davies-Bouldin Index for GMM\n",
    "        db_score_gmm = davies_bouldin_score(normalized_pca_components, gmm_labels)\n",
    "        db_index_gmm.append(db_score_gmm)\n",
    "        # Silhouette Score for GMM\n",
    "        silhouette_gmm.append(silhouette_score(normalized_pca_components, gmm_labels))\n",
    "        \n",
    "        print(f\"GMM - Number of components: {n}, Davies-Bouldin Index: {db_score_gmm}, Silhouette Score: {silhouette_score(normalized_pca_components, gmm_labels)}\")\n",
    "\n",
    "        # DBSCAN Clustering (DBSCAN does not require number of clusters, so we use eps and min_samples)\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        dbscan_labels = dbscan.fit_predict(normalized_pca_components)\n",
    "\n",
    "        # Davies-Bouldin Index for DBSCAN (ignoring -1 labels as noise points)\n",
    "        db_score_dbscan = davies_bouldin_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "        db_index_dbscan.append(db_score_dbscan)\n",
    "        \n",
    "        # Silhouette Score for DBSCAN (ignoring -1 labels as noise points)\n",
    "        silhouette_dbscan.append(silhouette_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1]))\n",
    "        \n",
    "        print(f\"DBSCAN - Davies-Bouldin Index: {db_score_dbscan}, Silhouette Score: {silhouette_score(normalized_pca_components[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])}\")\n",
    "\n",
    "    # Plot the Elbow graph and Davies-Bouldin Index in subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "    # Elbow Method Plot (Inertia)\n",
    "    axes[0].plot(range(2, n_clusters+1), silhouette_kmeans, marker='o')\n",
    "    axes[0].plot(range(2, n_clusters+1), silhouette_gmm, marker='o')\n",
    "    # axes[0].plot(range(2, n_clusters+1), silhouette_dbscan, marker='o')\n",
    "    axes[0].set_title(\"Silhouette Score for Different Number of Clusters\")\n",
    "    axes[0].set_xlabel(\"Number of Clusters\")\n",
    "    axes[0].set_ylabel(\"Silhouette Score\")\n",
    "    axes[0].legend(['KMeans', 'GMM', 'DBSCAN'])\n",
    "\n",
    "    # Davies-Bouldin Index Plot\n",
    "    axes[1].plot(range(2, n_clusters+1), db_index_kmeans, marker='o')\n",
    "    axes[1].plot(range(2, n_clusters+1), db_index_gmm, marker='o')\n",
    "    # axes[1].plot(range(2, n_clusters+1), db_index_dbscan, marker='o')\n",
    "    axes[1].set_title(\"Davies-Bouldin Index for Different Number of Clusters\")\n",
    "    axes[1].set_xlabel(\"Number of Clusters\")\n",
    "    axes[1].set_ylabel(\"Davies-Bouldin Index\")\n",
    "    axes[1].legend(['KMeans', 'GMM','DBSCAN'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the normalized PCA components\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, normalized_pca_components)\n",
    "\n",
    "n_clusters = 10\n",
    "# evaluate_clusters(normalized_pca_components, n_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
