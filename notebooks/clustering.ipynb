{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Add the root project directory to the Python path\n",
    "project_root = Path.cwd().parent  # This will get the project root since the notebook is in 'notebooks/'\n",
    "sys.path.append(str(project_root))\n",
    "from configs.path_config import CONFIG_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the strain data\n",
    "df_strain = pd.read_csv(OUTPUT_DIR / 'strain_distributions' / 'N-B_Far_Comp_20091129120000_20210611160000_strain_distribution.csv')\n",
    "df_strain.isna().sum().sum()\n",
    "# df_strain = df_strain.iloc[0:4700,:]\n",
    "df_strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers\n",
    "df_strain_data = df_strain.drop(columns = 'Timestamp')\n",
    "means = df_strain_data.mean(axis=1)  # Calculate mean for each row\n",
    "\n",
    "# Calculate the mean and standard deviation of the row means\n",
    "mean_val = means.mean()\n",
    "std_val = means.std()\n",
    "\n",
    "# Define a threshold for outliers (3 standard deviations from the mean)\n",
    "threshold = 1\n",
    "\n",
    "# Find outliers: rows where the absolute deviation from the mean is greater than the threshold\n",
    "outliers = means[np.abs(means - mean_val) > threshold * std_val]\n",
    "print(\"Outliers:\")\n",
    "print(outliers)\n",
    "\n",
    "df_strain = df_strain.drop(outliers.index)\n",
    "print(\"Number of outliers removed: \", len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = df_strain_data[df_strain_data.eq(0).all(axis=1)].index\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude timestamp in column 0\n",
    "strain_data = df_strain.iloc[:, 1:].values  \n",
    "strain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on the entire strain data (matrix-wise)\n",
    "# Set the number of components directly (e.g., 5 components)\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(strain_data)\n",
    "# \n",
    "# Get the explained variance ratio\n",
    "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(per_var) + 1), per_var.cumsum(), marker=\"o\", linestyle=\"--\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Percentage Cumulative of Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.xticks(range(1, len(per_var) + 1, 1))\n",
    "plt.title(\"Explained Variance by Number of Components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, strain_data):\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the entire strain data (matrix-wise)\n",
    "    pca.fit(strain_data)\n",
    "\n",
    "    # Apply PCA to the entire strain data (matrix-wise)\n",
    "    pca_results = pca.transform(strain_data)\n",
    "\n",
    "    # Normalize the results\n",
    "    normalized_pca_components = StandardScaler().fit_transform(pca_results)\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df_pca = pd.DataFrame(normalized_pca_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Add timestamps back\n",
    "    df_pca.insert(0, 'Timestamp', df_strain['Timestamp'].values)\n",
    "\n",
    "    return normalized_pca_components, df_pca\n",
    "\n",
    "n_components = 5\n",
    "normalized_pca_components, df_pca = do_pca(n_components, strain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(data, n_clusters):\n",
    "   \n",
    "    kmeans = KMeans(n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(normalized_pca_components)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Scatter plot with the clusters\n",
    "    sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + K-Means Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Add cluster labels to your original data (without overwriting)\n",
    "    data_with_KMeans = df_strain.copy()  # Make a copy to preserve the original DataFrame\n",
    "\n",
    "    # Insert the clusters as the second column (at index 1)\n",
    "    data_with_KMeans.insert(1, 'Cluster', clusters)\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_KMeans\n",
    "\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, strain_data)\n",
    "\n",
    "n_clusters = 8\n",
    "data_with_KMeans = kmeans_clustering(normalized_pca_components, n_clusters)\n",
    "data_with_KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(data, n_clusters):\n",
    "   \n",
    "    # Fit the Gaussian Mixture Model\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(normalized_pca_components)\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=normalized_pca_components[:, 0], y=normalized_pca_components[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + GMM Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Add the GMM cluster labels to the original data (without overwriting)\n",
    "    data_with_GMM = df_strain.copy()  # Make a copy to preserve the original DataFrame\n",
    "    data_with_GMM.insert(1, 'Cluster', clusters)  # Insert GMM clusters as the second column\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_GMM\n",
    "\n",
    "n_components = 8\n",
    "normalized_pca_components, df_pca = do_pca(n_components, strain_data)\n",
    "\n",
    "n_clusters = 8\n",
    "gmm_clustering(normalized_pca_components, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(data, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the provided data and visualize the results.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset to cluster.\n",
    "    - eps: The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "    - min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    \"\"\"\n",
    "    # Fit the DBSCAN model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(data)\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=clusters, palette=\"viridis\", s=100, alpha=0.7)\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"PCA + DBSCAN Clustering\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Add the DBSCAN cluster labels to the original data (without overwriting)\n",
    "    data_with_dbscan = df_strain.copy()  # Make a copy to preserve the original DataFrame\n",
    "    data_with_dbscan.insert(1, 'Cluster', clusters)  # Insert DBSCAN clusters as the second column\n",
    "\n",
    "    # Show the updated DataFrame with the Cluster column as the second column\n",
    "    return data_with_dbscan\n",
    "\n",
    "n_components = 5\n",
    "normalized_pca_components, df_pca = do_pca(n_components, strain_data)\n",
    "\n",
    "# Call the function with the normalized PCA components\n",
    "data_with_dbscan = dbscan_clustering(normalized_pca_components, eps=0.5, min_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(data, n_clusters) -> None:\n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for KMeans\n",
    "    silhouette_kmeans = []  # Sum of squared distances of samples to their closest cluster center\n",
    "    db_index_kmeans = []  # Davies-Bouldin Index\n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for GMM\n",
    "    silhouette_gmm = []  \n",
    "    db_index_gmm = []  \n",
    "\n",
    "    # Initialize lists to store inertia and Davies-Bouldin index for DBSCAN\n",
    "    silhouette_dbscan = []  \n",
    "    db_index_dbscan = []  \n",
    " \n",
    "\n",
    "    # Loop over different number of clusters\n",
    "    for n in range(2, n_clusters + 1):\n",
    "\n",
    "        # KMeans Clustering\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(data)\n",
    "\n",
    "        # Davies-Bouldin Index for KMeans\n",
    "        db_score_kmeans = davies_bouldin_score(data, kmeans_labels)\n",
    "        db_index_kmeans.append(db_score_kmeans)\n",
    "        # Silhouette Score for KMeans\n",
    "        silhouette_kmeans.append(silhouette_score(data, kmeans_labels))\n",
    "        \n",
    "        print(f\"K-Means - Number of clusters: {n}, Davies-Bouldin Index: {db_score_kmeans}, Silhouette Score: {silhouette_score(data, kmeans_labels)}\")\n",
    "\n",
    "        # GMM Clustering\n",
    "        gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "        gmm_labels = gmm.fit_predict(data)\n",
    "\n",
    "        # Davies-Bouldin Index for GMM\n",
    "        db_score_gmm = davies_bouldin_score(data, gmm_labels)\n",
    "        db_index_gmm.append(db_score_gmm)\n",
    "        # Silhouette Score for GMM\n",
    "        silhouette_gmm.append(silhouette_score(data, gmm_labels))\n",
    "        \n",
    "        print(f\"GMM - Number of components: {n}, Davies-Bouldin Index: {db_score_gmm}, Silhouette Score: {silhouette_score(data, gmm_labels)}\")\n",
    "\n",
    "        # DBSCAN Clustering (DBSCAN does not require number of clusters, so we use eps and min_samples)\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        dbscan_labels = dbscan.fit_predict(data)\n",
    "\n",
    "        # Davies-Bouldin Index for DBSCAN (ignoring -1 labels as noise points)\n",
    "        db_score_dbscan = davies_bouldin_score(data[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "        db_index_dbscan.append(db_score_dbscan)\n",
    "        \n",
    "        # Silhouette Score for DBSCAN (ignoring -1 labels as noise points)\n",
    "        silhouette_dbscan.append(silhouette_score(data[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1]))\n",
    "        \n",
    "        print(f\"DBSCAN - Davies-Bouldin Index: {db_score_dbscan}, Silhouette Score: {silhouette_score(data[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])}\")\n",
    "\n",
    "    # Plot the Elbow graph and Davies-Bouldin Index in subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "    # Elbow Method Plot (Inertia)\n",
    "    axes[0].plot(range(2, n_clusters+1), silhouette_kmeans, marker='o')\n",
    "    axes[0].plot(range(2, n_clusters+1), silhouette_gmm, marker='o')\n",
    "    axes[0].plot(range(2, n_clusters+1), silhouette_dbscan, marker='o')\n",
    "    axes[0].set_title(\"Silhouette Score for Different Number of Clusters\")\n",
    "    axes[0].set_xlabel(\"Number of Clusters\")\n",
    "    axes[0].set_ylabel(\"Silhouette Score\")\n",
    "    axes[0].legend(['KMeans', 'GMM', 'DBSCAN'])\n",
    "\n",
    "    # Davies-Bouldin Index Plot\n",
    "    axes[1].plot(range(2, n_clusters+1), db_index_kmeans, marker='o')\n",
    "    axes[1].plot(range(2, n_clusters+1), db_index_gmm, marker='o')\n",
    "    axes[1].plot(range(2, n_clusters+1), db_index_dbscan, marker='o')\n",
    "    axes[1].set_title(\"Davies-Bouldin Index for Different Number of Clusters\")\n",
    "    axes[1].set_xlabel(\"Number of Clusters\")\n",
    "    axes[1].set_ylabel(\"Davies-Bouldin Index\")\n",
    "    axes[1].legend(['KMeans', 'GMM','DBSCAN'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the normalized PCA components\n",
    "n_components = 5\n",
    "normalized_pca_components, df_pca = do_pca(n_components, strain_data)\n",
    "\n",
    "n_clusters = 6\n",
    "data = normalized_pca_components\n",
    "evaluate_clusters(data, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
